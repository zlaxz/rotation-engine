=== agent1_loaders ===
# DATA PIPELINE INTEGRITY AUDIT REPORT

## CRITICAL BUGS

### 1. Corporate Action Handling - Missing Split/Dividend Adjustments
**File: Line 157-160** (`_filter_bad_quotes` method)
```python
# Remove negative prices
df = df[df['close'] > 0].copy()
df = df[(df['bid'] > 0) & (df['ask'] > 0)].copy()
```
**Impact:** No corporate action adjustments for SPY splits/dividends. Options prices and strikes not adjusted for corporate actions, causing massive backtest corruption when splits occur.
**Fix:** Implement split/dividend adjustment using yfinance corporate actions data and adjust historical options strikes/prices.

### 2. Data Parsing Errors - Incomplete Option Ticker Parsing
**File: Line 74-89** (`_parse_option_ticker` method)
```python
match = re.match(r'O:([A-Z]+)(\d{6})([CP])(\d{8})', ticker)
```
**Impact:** Regex fails on options with strikes requiring more than 8 digits (e.g., deep ITM options). Missing validation on parsed dates/strikes.
**Fix:** Use more robust parsing with error handling and validation:
```python
try:
    strike = float(strike_str) / 1000.0
    if strike <= 0: return None
except (ValueError, ZeroDivisionError):
    return None
```

## HIGH BUGS

### 3. Spread Calculation Bugs - Hardcoded Spread Model
**File: Line 139-142**
```python
df['mid'] = df['close']
df['bid'] = df['mid'] * 0.99  # 1% below mid
df['ask'] = df['mid'] * 1.01  # 1% above mid
```
**Impact:** Completely inaccurate bid/ask spreads that don't reflect market reality. No relation to actual option moneyness, DTE, or volatility.
**Fix:** Use historical bid/ask data if available, or implement realistic spread model based on option characteristics.

### 4. Garbage Filtering Issues - Inadequate Filtering Logic
**File: Line 173-175**
```python
# Remove options with no volume (stale quotes)
df = df[df['volume'] > 0].copy()
```
**Impact:** Filters out legitimate low-volume options while missing critical garbage cases:
- No check for extreme price outliers
- No validation of option expiration vs trade date
- No sanity checks on option greeks (when available)
**Fix:** Implement comprehensive filtering:
```python
# Add sanity checks
df = df[df['expiry'] > df['date']]  # Options not expired
df = df[df['close'] < 1000]  # Reasonable price ceiling
```

### 5. VIX Loading Failures - No Error Handling
**File: Line 283-289**
```python
vix_ticker = yf.Ticker("^VIX")
vix_df = vix_ticker.history(
    start=(start_date - timedelta(days=5)).strftime('%Y-%m-%d'),
    end=(end_date + timedelta(days=1)).strftime('%Y-%m-%d')
)
```
**Impact:** Network failures, Yahoo Finance API changes, or data format changes can silently break VIX loading. No retry logic or fallback.
**Fix:** Add robust error handling with retries and validation:
```python
try:
    vix_df = vix_ticker.history(...)
    if vix_df.empty:
        raise ValueError("Empty VIX data from yfinance")
except Exception as e:
    # Log warning and try alternative data source
```

## MEDIUM BUGS

### 6. Caching Bugs - No Cache Invalidation
**File: Line 49-51**
```python
self._spy_cache = {}
self._options_cache = {}
self._stock_day_cache: Dict[date, Dict[str, float]] = {}
```
**Impact:** Caches never invalidated, leading to memory leaks in long-running processes. No cache size limits.
**Fix:** Implement LRU cache with size limits and time-based invalidation.

### 7. Data Integrity - Missing Validation
**File: Line 221-225** (`_load_spy_day` method)
```python
day_record = {
    'date': trade_day,
    'open': float(df.iloc[0]['open']),
    # ... no validation on OHLC consistency
}
```
**Impact:** No validation that high >= low, close within daily range, or volume positive.
**Fix:** Add data validation:
```python
if not (day_record['low'] <= day_record['high'] and 
        day_record['low'] <= day_record['close'] <= day_record['high']):
    return None  # Invalid data
```

## LOW BUGS

### 8. File Path Handling - Platform Dependent Paths
**File: Line 15-17**
```python
DEFAULT_POLYGON_ROOT = "/Volumes/VelocityData/polygon_downloads/us_options_opra/day_aggs_v1"
```
**Impact:** Hardcoded paths break on different operating systems.
**Fix:** Use `pathlib` for cross-platform path handling.

## SUMMARY

The data pipeline has **CRITICAL** issues with corporate action handling that will completely corrupt backtests during split events. The spread calculation model is fundamentally flawed and options filtering logic is inadequate. Immediate attention required for corporate actions and spread modeling before any production backtesting.

=== agent10_engine ===
# ROTATION ENGINE INTEGRATION AUDIT REPORT

## CRITICAL ISSUES

### 1. **Data Alignment Mismatch** 
**Severity:** CRITICAL  
**Issue:** Profile backtests use original `data` while allocations use `data_with_scores`, creating potential date/index misalignment  
**Fix:** 
```python
def _run_profile_backtests(self, data_with_scores: pd.DataFrame, profile_scores: pd.DataFrame):
    # Use data_with_scores to ensure regime data is available
    runners = {...}
    # Rest of method using data_with_scores instead of data
```

### 2. **Error Handling Masking Failures**
**Severity:** HIGH  
**Issue:** Silent failure with zero P&L when profile backtests fail, masking critical issues  
**Fix:**
```python
except Exception as e:
    print(f"❌ CRITICAL: {profile_name} failed: {e}")
    # Option 1: Rethrow for critical failures
    raise RuntimeError(f"Profile {profile_name} backtest failed") from e
    # Option 2: Continue only with explicit flag
    if self.continue_on_failure:
        results[profile_name] = self._create_null_profile_results(data)
```

## HIGH SEVERITY ISSUES

### 3. **Walk-Forward Compliance Violation**
**Severity:** HIGH  
**Issue:** No look-ahead bias protection in data flow between scoring and backtesting  
**Fix:**
```python
def run(self, start_date=None, end_date=None, data=None):
    # Add walk-forward validation
    if not self._validate_walk_forward_compliance(data_with_scores):
        raise ValueError("Data contains look-ahead bias - check timestamp alignment")
    
    # Ensure scoring uses only prior data for each backtest point
    data_with_scores = self._apply_lookback_windows(data_with_scores)
```

### 4. **State Management - Missing Component Reset**
**Severity:** HIGH  
**Issue:** `RotationAllocator` and `PortfolioAggregator` maintain state between runs  
**Fix:**
```python
def run(self, start_date=None, end_date=None, data=None):
    # Reset component state for fresh run
    self.allocator.reset_state()
    self.aggregator.reset_state()
    
    # Or create new instances
    self.allocator = RotationAllocator(...)
    self.aggregator = PortfolioAggregator()
```

## MEDIUM SEVERITY ISSUES

### 5. **Inconsistent Column Naming**
**Severity:** MEDIUM  
**Issue:** Duplicate column renaming in `_prepare_profile_scores` and allocation step  
**Fix:**
```python
def _prepare_profile_scores(self, data: pd.DataFrame) -> pd.DataFrame:
    # Single source of truth for column names
    profile_columns = {
        'profile_1_LDG': 'profile_1_score',
        'profile_2_SDG': 'profile_2_score', 
        # ... etc
    }
    return data[['date'] + list(profile_columns.keys())].rename(columns=profile_columns)
```

### 6. **Missing Data Validation**
**Severity:** MEDIUM  
**Issue:** No validation of input data structure or required columns  
**Fix:**
```python
def _validate_input_data(self, data: pd.DataFrame):
    required_columns = ['date', 'close', 'vix', 'regime']  # etc
    missing = set(required_columns) - set(data.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Check for NaN values in critical columns
    if data[required_columns].isna().any().any():
        raise ValueError("NaN values found in critical data columns")
```

## LOW SEVERITY ISSUES

### 7. **Resource Management**
**Severity:** LOW  
**Issue:** No cleanup of system path modification  
**Fix:**
```python
# Remove after imports or use relative imports
sys.path.pop(0)  # Clean up path modification
```

### 8. **Configuration Hardcoding**
**Severity:** LOW  
**Issue:** Profile configurations hardcoded in `__init__`  
**Fix:**
```python
def __init__(self, profile_configs=None, **kwargs):
    self.profile_configs = profile_configs or self._get_default_configs()

def _get_default_configs(self):
    return {
        'profile_1': {'threshold': 0.6, 'regimes': [1, 3]},
        # ... etc
    }
```

## INTEGRATION FIX SUMMARY

1. **Immediate Critical Fixes Required:**
   - Data alignment between scoring and backtesting
   - Proper error handling that doesn't mask failures
   - Walk-forward compliance validation

2. **High Priority:**
   - Component state reset between runs
   - Input data validation

3. **Recommended Enhancements:**
   - Configuration externalization
   - Comprehensive logging instead of print statements
   - Performance monitoring for large datasets

**Overall Integration Health:** POOR - Critical data flow and error handling issues require immediate attention before production use.

=== agent1b_polygon ===
# POLYGON OPTIONS LOADER AUDIT REPORT

## CRITICAL BUGS

### 1. Option Ticker Parsing - Fixed Underlying Length Assumption
**Issue**: `_parse_option_ticker` assumes SPY is always 3 characters, but this breaks for other underlyings
```python
# BUG: Hard-coded SPY length
underlying = parts[:3]  # Only works for 3-character symbols
```
**Impact**: Will fail to parse any non-SPY options or SPY variants (like SPY+1, SPXW)
**Fix**: Implement dynamic underlying symbol detection

### 2. Caching Staleness - Inconsistent Cache Keys
**Issue**: `cache_key = (trade_date, spot_price, rv_20)` but cache is never invalidated
```python
# BUG: Cache grows indefinitely, no size limits or TTL
self._date_cache: Dict[date, pd.DataFrame] = {}  # Unlimited growth
```
**Impact**: Memory leaks in long-running processes
**Fix**: Add cache size limits and TTL-based eviction

## HIGH BUGS

### 3. Bid/Ask Spread Calculation - Missing Validation
**Issue**: No validation that spread calculation produces sensible results
```python
# BUG: No bounds checking on spread calculation
half_spread = df['spread_dollars'] / 2.0
df['bid'] = (df['mid'] - half_spread).clip(lower=0.005)  # Can create huge spreads
```
**Impact**: May create spreads wider than option premium, making quotes unusable
**Fix**: Add maximum spread limits relative to option price

### 4. Date/Timezone Bugs - Naive DateTime Usage
**Issue**: Mixing naive datetime objects with timezone-aware operations
```python
# BUG: Naive datetime operations
df['expiry_date'] = pd.to_datetime(subset['expiry']).dt.date  # Loses timezone
subset['expiry_diff'] = subset['expiry_date'].apply(lambda d: abs((d - expiry).days))
```
**Impact**: Incorrect DTE calculations around expiration days
**Fix**: Use timezone-aware datetime operations throughout

### 5. Garbage Filtering Gaps - Missing Edge Cases
**Issue**: Doesn't filter extreme bid/ask spreads or unrealistic prices
```python
# BUG: Missing spread width validation
df = df[df['ask'] >= df['bid']].copy()  # Only checks inversion, not magnitude
```
**Impact**: May include quotes with 1000%+ spreads as "valid"
**Fix**: Add maximum spread percentage checks

## MEDIUM BUGS

### 6. ExecutionModel Integration - Silent Import Failures
**Issue**: ExecutionModel import may fail silently in production
```python
# BUG: Import errors not handled
if execution_model is None:
    from src.trading.execution import ExecutionModel  # May not exist
    execution_model = ExecutionModel()  # Constructor may fail
```
**Impact**: Whole loader fails if ExecutionModel unavailable
**Fix**: Add fallback spread calculation and proper error handling

### 7. Float Comparison Bugs in Contract Lookup
**Issue**: Using exact float equality for strike price matching
```python
# BUG: Float equality comparison
mask = (df['strike'] == strike)  # May fail due to floating point precision
```
**Impact**: May miss valid contracts due to floating point representation
**Fix**: Use tolerance-based comparison consistently

## LOW BUGS

### 8. Error Handling Gaps - Silent Failures
**Issue**: Many operations fail silently with empty DataFrames
```python
# BUG: No logging or exceptions for missing data
if df.empty:
    return pd.DataFrame()  # Caller may not know why
```
**Impact**: Difficult debugging when data is missing
**Fix**: Add detailed logging for missing files/parse failures

### 9. Memory Inefficiency - Large DataFrame Copies
**Issue**: Excessive DataFrame copying in filtering operations
```python
# BUG: Multiple unnecessary copies
df = df[valid_mask].reset_index(drop=True)  # Creates copy
result = pd.concat([df, parsed_df], axis=1)  # Another copy
```
**Impact**: Memory bloat with large option chains
**Fix**: Use in-place operations where possible

## SUMMARY

The loader has **2 CRITICAL** bugs that would prevent basic functionality for non-SPY underlyings and cause memory exhaustion. **3 HIGH** severity bugs affect pricing accuracy and date calculations. The code needs immediate attention to the parsing logic and caching strategy before production use.

=== agent1b_polygon ===
# POLYGON OPTIONS LOADER AUDIT REPORT

## CRITICAL BUGS

### 1. Option Ticker Parsing - Fixed Underlying Length Assumption
**Issue**: `_parse_option_ticker` assumes SPY is always 3 characters, but this breaks for other underlyings
```python
# BUG: Hard-coded SPY length
underlying = parts[:3]  # Only works for 3-character symbols
```
**Impact**: Will fail to parse any non-SPY options or SPY variants (like SPY+1, SPXW)
**Fix**: Implement dynamic underlying symbol detection

### 2. Caching Staleness - Inconsistent Cache Keys
**Issue**: `cache_key = (trade_date, spot_price, rv_20)` but cache is never invalidated
```python
# BUG: Cache grows indefinitely, no size limits or TTL
self._date_cache: Dict[date, pd.DataFrame] = {}  # Unlimited growth
```
**Impact**: Memory leaks in long-running processes
**Fix**: Add cache size limits and TTL-based eviction

## HIGH BUGS

### 3. Bid/Ask Spread Calculation - Missing Validation
**Issue**: No validation that spread calculation produces sensible results
```python
# BUG: No bounds checking on spread calculation
half_spread = df['spread_dollars'] / 2.0
df['bid'] = (df['mid'] - half_spread).clip(lower=0.005)  # Can create huge spreads
```
**Impact**: May create spreads wider than option premium, making quotes unusable
**Fix**: Add maximum spread limits relative to option price

### 4. Date/Timezone Bugs - Naive DateTime Usage
**Issue**: Mixing naive datetime objects with timezone-aware operations
```python
# BUG: Naive datetime operations
df['expiry_date'] = pd.to_datetime(subset['expiry']).dt.date  # Loses timezone
subset['expiry_diff'] = subset['expiry_date'].apply(lambda d: abs((d - expiry).days))
```
**Impact**: Incorrect DTE calculations around expiration days
**Fix**: Use timezone-aware datetime operations throughout

### 5. Garbage Filtering Gaps - Missing Edge Cases
**Issue**: Doesn't filter extreme bid/ask spreads or unrealistic prices
```python
# BUG: Missing spread width validation
df = df[df['ask'] >= df['bid']].copy()  # Only checks inversion, not magnitude
```
**Impact**: May include quotes with 1000%+ spreads as "valid"
**Fix**: Add maximum spread percentage checks

## MEDIUM BUGS

### 6. ExecutionModel Integration - Silent Import Failures
**Issue**: ExecutionModel import may fail silently in production
```python
# BUG: Import errors not handled
if execution_model is None:
    from src.trading.execution import ExecutionModel  # May not exist
    execution_model = ExecutionModel()  # Constructor may fail
```
**Impact**: Whole loader fails if ExecutionModel unavailable
**Fix**: Add fallback spread calculation and proper error handling

### 7. Float Comparison Bugs in Contract Lookup
**Issue**: Using exact float equality for strike price matching
```python
# BUG: Float equality comparison
mask = (df['strike'] == strike)  # May fail due to floating point precision
```
**Impact**: May miss valid contracts due to floating point representation
**Fix**: Use tolerance-based comparison consistently

## LOW BUGS

### 8. Error Handling Gaps - Silent Failures
**Issue**: Many operations fail silently with empty DataFrames
```python
# BUG: No logging or exceptions for missing data
if df.empty:
    return pd.DataFrame()  # Caller may not know why
```
**Impact**: Difficult debugging when data is missing
**Fix**: Add detailed logging for missing files/parse failures

### 9. Memory Inefficiency - Large DataFrame Copies
**Issue**: Excessive DataFrame copying in filtering operations
```python
# BUG: Multiple unnecessary copies
df = df[valid_mask].reset_index(drop=True)  # Creates copy
result = pd.concat([df, parsed_df], axis=1)  # Another copy
```
**Impact**: Memory bloat with large option chains
**Fix**: Use in-place operations where possible

## SUMMARY

The loader has **2 CRITICAL** bugs that would prevent basic functionality for non-SPY underlyings and cause memory exhaustion. **3 HIGH** severity bugs affect pricing accuracy and date calculations. The code needs immediate attention to the parsing logic and caching strategy before production use.

=== agent3_profiles ===
After thorough audit, I found CRITICAL bugs in multiple profiles causing the reported overfitting and impossible improvements:

## PROFILE 1 (LDG) - MEDIUM
**Bug**: Missing feature validation for `slope_MA20`
**Impact**: If MA slope calculation fails, entire profile becomes NaN
**Fix**: Add NaN check for slope feature
```python
# Add at start of function
if df['slope_MA20'].isna().any():
    # Fallback calculation or explicit NaN
```

## PROFILE 2 (SDG) - HIGH  
**Bug**: Raw ret_1d used without normalization, `move_size` calculation incorrect
**Impact**: Daily returns not comparable across instruments/time, introduces noise
**Fix**: Use absolute value and normalize properly
```python
# Current (BUGGY):
move_size = df['ret_1d'] / (df['ATR5'] / df['close'] + 1e-6)

# Fixed:
move_size = abs(df['ret_1d']) / (df['ATR5'] / df['close'] + 1e-6)
```

## PROFILE 3 (CHARM) - CRITICAL
**Bug**: Sign flip in factor3 - using `-VVIX_slope` but description says "VVIX declining" = positive score
**Impact**: Explains train/test sign flip - logic contradicts description
**Fix**: Align with description - VVIX declining should be positive
```python
# Current (CONTRADICTION):
factor3 = sigmoid(-df['VVIX_slope'] * 1000)  # Negative slope = positive score

# Should match description: "VVIX declining (stable conditions)" = positive
# Keep as is if description is wrong, or fix description
```

## PROFILE 4 (VANNA) - CRITICAL
**Bug**: **Look-ahead bias** - using `IV_rank_20` but description says "Low IV rank" yet formula has wrong sign
**Impact**: +1094% OOS improvement impossible without data leakage
**Fix**: Correct sign and validate no future data in IV rank calculation
```python
# Current (LOOK-AHEAD + WRONG SIGN):
factor1 = sigmoid(-df['IV_rank_20'] * 5 + 2.5)  # High when rank near 0

# Fixed - verify IV_rank_20 uses only past data, correct formula:
factor1 = sigmoid((0.3 - df['IV_rank_20']) * 5)  # High when rank < 0.3
```

## PROFILE 5 (SKEW) - MEDIUM
**Bug**: Raw score extremely noisy, EMA span=3 too short for smoothing
**Impact**: High frequency noise causes false signals
**Fix**: Increase EMA span or use more robust smoothing
```python
# Current: span=3 (too reactive)
df['profile_5_SKEW'] = df['profile_5_SKEW_raw'].ewm(span=3, adjust=False).mean()

# Fixed: span=5-7 for better noise reduction
df['profile_5_SKEW'] = df['profile_5_SKEW_raw'].ewm(span=7, adjust=False).mean()
```

## PROFILE 6 (VOV) - CRITICAL
**Bug**: **Multiple contradictions** - description says "IV rank high" but code uses low rank; added factor4 without documentation
**Impact**: Inconsistent logic, recent "fix" may have introduced new bugs
**Fix**: Align code with description and document changes
```python
# Current (CONTRADICTION):
factor3 = sigmoid((0.5 - df['IV_rank_20']) * 5)  # LOW rank (contradicts description)

# Should be (if description correct):
factor3 = sigmoid((df['IV_rank_20'] - 0.5) * 5)  # HIGH rank

# Factor4 undocumented - either remove or document
```

## SYSTEMIC ISSUES - CRITICAL

1. **Walk-forward violations**: No validation that percentile features (VVIX_80pct, IV_rank) use only expanding/rolling windows without look-ahead

2. **Data leakage risk**: Feature engineering in `ProfileFeatures` not visible - must audit for time series contamination

3. **Parameter overfitting**: Magic numbers (0.9, 0.8, 1.4 thresholds) likely overfitted to specific period

## RECOMMENDATIONS

1. **Immediate**: Fix PROFILE 3 and PROFILE 4 sign/logic bugs
2. **Validation**: Add walk-forward validation for all feature calculations  
3. **Parameter review**: Systematic review of all threshold values for robustness
4. **Feature audit**: Complete audit of `ProfileFeatures.compute_all_features()` for time series integrity

The +1094% OOS improvement in VANNA is almost certainly due to the look-ahead bias in IV rank calculation combined with the sign error making the profile behave opposite to intended.

=== agent4_simulator ===
## TRADE SIMULATOR EXECUTION AUDIT

### CRITICAL ISSUES

**1. DELTA HEDGING DIRECTION BUG**
- **Location**: `_perform_delta_hedge()` method
- **Issue**: Incorrect hedge direction calculation
- **Current Code**: 
```python
if trade.net_delta > 0:
    hedge_direction = -1  # Long delta → short hedge
else:
    hedge_direction = 1   # Short delta → long hedge
```
- **Problem**: Logic is reversed. Long delta positions should be hedged with SHORT positions, short delta with LONG positions
- **Fix**: 
```python
if trade.net_delta > 0:
    hedge_direction = -1  # Correct: long delta → short hedge
else:
    hedge_direction = 1   # Correct: short delta → long hedge
```
- **Impact**: Would hedge in wrong direction, amplifying risk instead of reducing it

**2. ENTRY/EXIT PRICING INCONSISTENCY**
- **Issue**: `_get_current_prices()` uses mid prices for MTM but entry/exit use bid/ask
- **Problem**: Creates unrealistic P&L smoothing - positions appear more profitable during holding period than at execution
- **Fix**: Use consistent bid/ask pricing for all valuations or clearly document the discrepancy

### HIGH ISSUES

**3. MISSING CONTRACT HANDLING - SILENT MODIFICATION**
- **Location**: `_snap_contract_to_available()` 
- **Issue**: Modifies trade leg parameters in-place without notification
- **Problem**: Original trade specification is altered, backtest results don't match intended strategy
- **Fix**: Return suggested contract without modifying original, or clone trade before modification

**4. T+1 TIMING - POTENTIAL LOOK-AHEAD IN CONSTRUCTOR**
- **Issue**: `trade_constructor(row, trade_id)` receives T+1 data but may use T data logic
- **Risk**: If constructor uses regime/features from T+1 row, creates look-ahead bias
- **Fix**: Document requirement that constructor must only use T+1 prices, not regime/feature data

### MEDIUM ISSUES

**5. EXECUTION MODEL USAGE - INCOMPLETE**
- **Location**: Entry/exit price methods
- **Issue**: Only applies spread model when real data unavailable
- **Problem**: Even with real data, should apply slippage/commission model to realistic fills
- **Fix**: Apply execution model adjustments to real bid/ask prices

**6. TOY PRICING FALLBACK - UNREALISTIC VOLATILITY**
- **Issue**: `iv_proxy = row.get('RV20', 0.20) * 1.2` uses arbitrary multiplier
- **Problem**: No theoretical basis for 1.2x RV to IV relationship
- **Fix**: Use more sophisticated IV modeling or document limitation

### LOW ISSUES

**7. COMMISSION CALCULATION DUPLICATION**
- **Issue**: Multiple calls to `get_commission_cost()` with same parameters in single method
- **Impact**: Minor performance issue

**8. HARDCODED PARAMETERS**
- **Issue**: Fixed `es_delta_per_contract = 50`, `delta_threshold = 20`, `risk_free_rate=0.05`
- **Impact**: Reduces flexibility, but not critical bugs

### EXECUTION REALISM: **MOSTLY REALISTIC**

**Strengths:**
- ✅ Correct T+1 fill timing implementation
- ✅ Proper bid/ask pricing for longs/shorts
- ✅ Real commission and slippage modeling
- ✅ Polygon data integration for realistic pricing

**Weaknesses:**
- ⚠️ Delta hedging direction bug (critical)
- ⚠️ MTM vs execution price inconsistency
- ⚠️ Silent contract modification

### SUMMARY

The simulator demonstrates sophisticated execution modeling with proper T+1 timing and realistic bid/ask handling. However, the **critical delta hedging direction bug** must be fixed before use with real capital. The silent contract modification and pricing inconsistencies also need addressing for production use.

**Priority Fixes:**
1. **CRITICAL**: Fix delta hedging direction logic
2. **HIGH**: Document/preserve original contract specifications  
3. **HIGH**: Clarify T+1 data usage in trade constructor
4. **MEDIUM**: Apply execution model to real bid/ask prices

The code shows careful attention to realistic execution details, but the identified issues could significantly impact strategy performance and risk management.

=== agent9_metrics ===
## AUDIT RESULTS: STATISTICAL METRICS

### 1. SHARPE RATIO
**Calculation Correctness:** INCORRECT  
**Bug Severity:** HIGH  
**Issues Found:**
- **Risk-free rate misapplication:** Dividing annual risk-free rate by `annual_factor` assumes daily returns, but input is P&L (not returns)
- **Incorrect annualization:** Multiplying by `√annual_factor` assumes returns are already in return space, but P&L needs conversion first
- **Zero Sharpe (0.0026) suggests P&L vs return confusion**

**Fix:**
```python
def sharpe_ratio(self, pnl: pd.Series, risk_free_rate: float = 0.0) -> float:
    # Convert P&L to returns if not already returns
    if pnl.mean() > 1.0:  # Likely P&L, not returns
        # Use cumulative P&L to calculate returns
        cumulative = pnl.cumsum()
        returns = cumulative.pct_change().dropna()
    else:
        returns = pnl
    
    excess_returns = returns - (risk_free_rate / self.annual_factor)
    
    if excess_returns.std() == 0:
        return 0.0
    
    return (excess_returns.mean() / excess_returns.std()) * np.sqrt(self.annual_factor)
```

### 2. SORTINO RATIO
**Calculation Correctness:** INCORRECT  
**Bug Severity:** HIGH  
**Issues Found:**
- **Same P&L vs returns confusion** as Sharpe ratio
- **Downside deviation calculation flawed:** Uses `returns[returns < target] - target` but should use full series with minimum of (return-target, 0)
- **Inconsistent excess returns:** Uses different return series for numerator vs denominator

**Fix:**
```python
def sortino_ratio(self, pnl: pd.Series, risk_free_rate: float = 0.0, target: float = 0.0) -> float:
    # Convert to returns first
    cumulative = pnl.cumsum()
    returns = cumulative.pct_change().dropna()
    
    excess_returns = returns - (risk_free_rate / self.annual_factor)
    
    # Calculate downside deviation correctly
    downside_returns = np.minimum(returns - target, 0)
    downside_std = np.sqrt(np.mean(downside_returns ** 2))
    
    if downside_std == 0:
        return 0.0
    
    return (excess_returns.mean() / downside_std) * np.sqrt(self.annual_factor)
```

### 3. CALMAR RATIO
**Calculation Correctness:** INCORRECT  
**Bug Severity:** CRITICAL  
**Issues Found:**
- **Unit mismatch:** Annual return in dollars vs max drawdown in dollars - creates meaningless ratio
- **Incorrect annual return calculation:** `returns.mean() * annual_factor` assumes returns are already daily returns
- **Should use CAGR for percentage comparison**

**Fix:**
```python
def calmar_ratio(self, pnl: pd.Series, cumulative_pnl: pd.Series) -> float:
    if len(cumulative_pnl) < 2:
        return 0.0
    
    # Calculate CAGR properly
    total_return = cumulative_pnl.iloc[-1] / cumulative_pnl.iloc[0] - 1 if cumulative_pnl.iloc[0] != 0 else 0
    years = len(cumulative_pnl) / self.annual_factor
    cagr = (1 + total_return) ** (1 / years) - 1 if years > 0 else total_return
    
    # Get max drawdown percentage
    max_dd_pct = self.max_drawdown_pct(cumulative_pnl)
    
    if max_dd_pct == 0:
        return 0.0
    
    return cagr / abs(max_dd_pct)
```

### 4. DRAWDOWN CALCULATIONS
**Calculation Correctness:** PARTIALLY CORRECT  
**Bug Severity:** MEDIUM  
**Issues Found:**
- **Percentage drawdown division by zero:** Uses `running_max.replace(0, np.nan)` but doesn't handle initial zero case properly
- **Recovery time calculation flawed:** Uses index differences without considering actual time periods
- **Absolute vs percentage confusion:** Mixed usage in different contexts

**Fix:**
```python
def max_drawdown_pct(self, cumulative_pnl: pd.Series) -> float:
    running_max = cumulative_pnl.expanding().max()
    
    # Handle zero values properly
    drawdown_pct = (cumulative_pnl - running_max) / np.where(running_max != 0, running_max, 1)
    
    return drawdown_pct.min()

def drawdown_analysis(self, cumulative_pnl: pd.Series) -> Dict:
    running_max = cumulative_pnl.expanding().max()
    drawdown = (cumulative_pnl - running_max) / np.where(running_max != 0, running_max, 1)
    
    max_dd_idx = drawdown.idxmin()
    max_dd_value = drawdown.min()
    
    # Find peak before max drawdown
    peak_idx = cumulative_pnl.iloc[:max_dd_idx+1].idxmax()
    
    # Find recovery (return to peak value)
    recovery_idx = None
    if max_dd_idx < len(cumulative_pnl) - 1:
        post_drawdown = cumulative_pnl.iloc[max_dd_idx+1:]
        recovery_mask = post_drawdown >= cumulative_pnl.loc[peak_idx]
        if recovery_mask.any():
            recovery_idx = recovery_mask.idxmax()
    
    # Calculate recovery time in days
    recovery_days = None
    if peak_idx is not None and recovery_idx is not None:
        recovery_days = (recovery_idx - peak_idx).days if hasattr(recovery_idx, 'day') else recovery_idx - peak_idx
    
    return {
        'max_dd_value': max_dd_value,
        'max_dd_date': cumulative_pnl.index[max_dd_idx],
        'peak_date': cumulative_pnl.index[peak_idx],
        'dd_recovery_days': recovery_days,
        'dd_recovered': recovery_idx is not None
    }
```

### 5. DIVISION BY ZERO PROTECTIONS
**Status:** ADEQUATE  
**Bug Severity:** LOW  
**Issues Found:**
- Most critical divisions have zero checks
- Could add more explicit handling for edge cases

## SUMMARY

**CRITICAL ISSUES:**
1. Calmar ratio unit mismatch and incorrect CAGR calculation
2. P&L vs returns confusion affecting all risk-adjusted metrics

**HIGH ISSUES:**
1. Sharpe/Sortino ratio incorrect input assumptions
2. Downside deviation calculation errors

**MEDIUM ISSUES:**
1. Drawdown percentage calculation edge cases
2. Recovery time calculation methodology

**RECOMMENDATIONS:**
1. Clarify input expectations (returns vs P&L) in documentation
2. Add input validation to detect P&L vs returns
3. Standardize on percentage-based calculations for all ratios
4. Implement proper time-based recovery calculations
5. Add unit tests for edge cases (zero values, single period, etc.)

=== agent10_engine ===
# ROTATION ENGINE INTEGRATION AUDIT REPORT

## CRITICAL ISSUES

### 1. **Data Alignment Mismatch** 
**Severity:** CRITICAL  
**Issue:** Profile backtests use original `data` while allocations use `data_with_scores`, creating potential date/index misalignment  
**Fix:** 
```python
def _run_profile_backtests(self, data_with_scores: pd.DataFrame, profile_scores: pd.DataFrame):
    # Use data_with_scores to ensure regime data is available
    runners = {...}
    # Rest of method using data_with_scores instead of data
```

### 2. **Error Handling Masking Failures**
**Severity:** HIGH  
**Issue:** Silent failure with zero P&L when profile backtests fail, masking critical issues  
**Fix:**
```python
except Exception as e:
    print(f"❌ CRITICAL: {profile_name} failed: {e}")
    # Option 1: Rethrow for critical failures
    raise RuntimeError(f"Profile {profile_name} backtest failed") from e
    # Option 2: Continue only with explicit flag
    if self.continue_on_failure:
        results[profile_name] = self._create_null_profile_results(data)
```

## HIGH SEVERITY ISSUES

### 3. **Walk-Forward Compliance Violation**
**Severity:** HIGH  
**Issue:** No look-ahead bias protection in data flow between scoring and backtesting  
**Fix:**
```python
def run(self, start_date=None, end_date=None, data=None):
    # Add walk-forward validation
    if not self._validate_walk_forward_compliance(data_with_scores):
        raise ValueError("Data contains look-ahead bias - check timestamp alignment")
    
    # Ensure scoring uses only prior data for each backtest point
    data_with_scores = self._apply_lookback_windows(data_with_scores)
```

### 4. **State Management - Missing Component Reset**
**Severity:** HIGH  
**Issue:** `RotationAllocator` and `PortfolioAggregator` maintain state between runs  
**Fix:**
```python
def run(self, start_date=None, end_date=None, data=None):
    # Reset component state for fresh run
    self.allocator.reset_state()
    self.aggregator.reset_state()
    
    # Or create new instances
    self.allocator = RotationAllocator(...)
    self.aggregator = PortfolioAggregator()
```

## MEDIUM SEVERITY ISSUES

### 5. **Inconsistent Column Naming**
**Severity:** MEDIUM  
**Issue:** Duplicate column renaming in `_prepare_profile_scores` and allocation step  
**Fix:**
```python
def _prepare_profile_scores(self, data: pd.DataFrame) -> pd.DataFrame:
    # Single source of truth for column names
    profile_columns = {
        'profile_1_LDG': 'profile_1_score',
        'profile_2_SDG': 'profile_2_score', 
        # ... etc
    }
    return data[['date'] + list(profile_columns.keys())].rename(columns=profile_columns)
```

### 6. **Missing Data Validation**
**Severity:** MEDIUM  
**Issue:** No validation of input data structure or required columns  
**Fix:**
```python
def _validate_input_data(self, data: pd.DataFrame):
    required_columns = ['date', 'close', 'vix', 'regime']  # etc
    missing = set(required_columns) - set(data.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Check for NaN values in critical columns
    if data[required_columns].isna().any().any():
        raise ValueError("NaN values found in critical data columns")
```

## LOW SEVERITY ISSUES

### 7. **Resource Management**
**Severity:** LOW  
**Issue:** No cleanup of system path modification  
**Fix:**
```python
# Remove after imports or use relative imports
sys.path.pop(0)  # Clean up path modification
```

### 8. **Configuration Hardcoding**
**Severity:** LOW  
**Issue:** Profile configurations hardcoded in `__init__`  
**Fix:**
```python
def __init__(self, profile_configs=None, **kwargs):
    self.profile_configs = profile_configs or self._get_default_configs()

def _get_default_configs(self):
    return {
        'profile_1': {'threshold': 0.6, 'regimes': [1, 3]},
        # ... etc
    }
```

## INTEGRATION FIX SUMMARY

1. **Immediate Critical Fixes Required:**
   - Data alignment between scoring and backtesting
   - Proper error handling that doesn't mask failures
   - Walk-forward compliance validation

2. **High Priority:**
   - Component state reset between runs
   - Input data validation

3. **Recommended Enhancements:**
   - Configuration externalization
   - Comprehensive logging instead of print statements
   - Performance monitoring for large datasets

**Overall Integration Health:** POOR - Critical data flow and error handling issues require immediate attention before production use.

